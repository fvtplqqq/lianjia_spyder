# lianjia_selenium_crawler.py
import time
import random
import json
import os
import re
import pandas as pd
from datetime import datetime
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# é…ç½®è·¯å¾„
CONFIG_FILE = 'config.json'
DATA_DIR = 'data'
os.makedirs(DATA_DIR, exist_ok=True)
OUTPUT_FILE = os.path.join(DATA_DIR, f'é“¾å®¶ç§Ÿæˆ¿æ•°æ®_Selenium_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx')


def init_driver():
    """åˆå§‹åŒ–å¸¦æŒä¹…åŒ–é…ç½®çš„ Chrome æµè§ˆå™¨"""
    chrome_options = Options()
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_argument("user-data-dir=C:\\Temp\\LianjiaProfile_Selenium")  # ä¿å­˜ç™»å½•/éªŒè¯çŠ¶æ€

    # é™é»˜æ¨¡å¼ï¼ˆå¯é€‰ï¼‰ï¼šå–æ¶ˆä¸‹é¢ä¸¤è¡Œæ³¨é‡Šå¯åå°è¿è¡Œï¼ˆä½†æ— æ³•äººå·¥è¿‡éªŒè¯ï¼ï¼‰
    # chrome_options.add_argument("--headless")
    # chrome_options.add_argument("--disable-gpu")

    try:
        from webdriver_manager.chrome import ChromeDriverManager
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_options
        )
        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
            'source': '''
                delete navigator.__proto__.webdriver;
                window.navigator.permissions.query = (parameters) => {
                    return parameters.name === 'notifications' ?
                        Promise.resolve({ state: Notification.permission }) :
                        originalQuery(parameters);
                };
            '''
        })
        return driver
    except Exception as e:
        print(f"åˆå§‹åŒ–æµè§ˆå™¨å¤±è´¥: {e}")
        return None


def load_config():
    if not Path(CONFIG_FILE).exists():
        default_config = {
            "urls": [
                "https://sh.lianjia.com/zufang/jingan/rco11rt200600000001ra1ra2ra3ra4ra5rp6rp7rp4rp5",
                "https://sh.lianjia.com/zufang/xuhui/rco11rt200600000001ra1ra2ra3ra4ra5rp6rp7rp4rp5"
            ],
            "max_pages": 5,
            "delay": 3
        }
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=2)
        print(f"å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ {CONFIG_FILE}")

    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config['urls'], config.get('max_pages', 5), config.get('delay', 1)


# ========== ä¿ç•™ä½ åŸæœ‰çš„è§£æå‡½æ•° ==========

def extract_location_info(des_tag):
    location_data = {'ä¸€çº§åŒºåŸŸ': '', 'äºŒçº§åŒºåŸŸ': '', 'å°åŒºåç§°': '', 'å°åŒºé“¾æ¥': ''}
    if des_tag:
        try:
            links = des_tag.find_all('a')
            if len(links) >= 1:
                location_data['ä¸€çº§åŒºåŸŸ'] = links[0].get_text(strip=True)
            if len(links) >= 2:
                location_data['äºŒçº§åŒºåŸŸ'] = links[1].get_text(strip=True)
            if len(links) >= 3:
                location_data['å°åŒºåç§°'] = links[2].get_text(strip=True)
                location_data['å°åŒºé“¾æ¥'] = 'https://sh.lianjia.com' + links[2]['href']
        except Exception as e:
            print(f"æå–ä½ç½®ä¿¡æ¯å‡ºé”™: {str(e)}")
    return location_data


def parse_house(house) -> dict:
    data = {}
    try:
        title_tag = house.find('a', class_='content__list--item--aside')
        data['æ ‡é¢˜'] = title_tag.get('title', '').strip() if title_tag else ''
        data['é“¾æ¥'] = 'https://sh.lianjia.com' + title_tag.get('href', '').strip() if title_tag else ''

        price_tag = house.find('span', class_='content__list--item-price')
        if price_tag:
            price_text = price_tag.get_text(strip=True)
            data['ä»·æ ¼(å…ƒ)'] = int(''.join(filter(str.isdigit, price_text)))
            data['ä»·æ ¼å•ä½'] = price_text.replace(str(data['ä»·æ ¼(å…ƒ)']), '').strip()

        des_tag = house.find('p', class_='content__list--item--des')
        data.update(extract_location_info(des_tag))

        if des_tag:
            features = [f.strip() for f in des_tag.stripped_strings if f.strip() not in ['-', '/']]
            for item in features:
                if 'ã¡' in item:
                    data['é¢ç§¯(ã¡)'] = float(''.join(filter(lambda x: x.isdigit() or x == '.', item)))
                elif any(c in item for c in ['ä¸œ', 'å—', 'è¥¿', 'åŒ—']):
                    data['æœå‘'] = item
                elif any(c in item for c in ['å®¤', 'å…', 'å«']):
                    data['æˆ·å‹'] = item
                elif 'å±‚' in item:
                    data['æ¥¼å±‚'] = item
                    if 'ï¼ˆ' in item and 'ï¼‰' in item:
                        nums = re.findall(r'(\d+)å±‚', item)
                        if nums:
                            data['æ€»æ¥¼å±‚'] = int(nums[-1])
                elif 'å¹´å»º' in item:
                    data['å»ºæˆå¹´ä»½'] = int(''.join(filter(str.isdigit, item)))

        tags = house.find('p', class_='content__list--item--bottom')
        if tags:
            tag_list = [tag.get_text(strip=True) for tag in tags.find_all('i')]
            data['æ ‡ç­¾'] = '|'.join(tag_list)
            data['å®˜æ–¹æ ¸éªŒ'] = 'å®˜æ–¹æ ¸éªŒ' in tag_list
            data['è¿‘åœ°é“'] = 'è¿‘åœ°é“' in tag_list
            data['ç²¾è£…'] = 'ç²¾è£…' in tag_list

        brand_tag = house.find('p', class_='content__list--item--brand')
        if brand_tag:
            data['ä¸­ä»‹å…¬å¸'] = brand_tag.find('span', class_='brand').get_text(strip=True) if brand_tag.find('span',
                                                                                                             class_='brand') else ''
            data['ç»´æŠ¤æ—¶é—´'] = brand_tag.find('span', class_='content__list--item--time').get_text(
                strip=True) if brand_tag.find('span', class_='content__list--item--time') else ''

        data['å¿…çœ‹å¥½æˆ¿'] = bool(house.find('img', alt='å¿…çœ‹å¥½æˆ¿'))
        data['VRçœ‹æˆ¿'] = bool(house.find('i', class_='vr-logo'))
        data['çˆ¬å–æ—¶é—´'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    except Exception as e:
        print(f"è§£ææˆ¿æºå‡ºé”™: {str(e)}")
    return data


def save_to_excel(df: pd.DataFrame, filename: str):
    try:
        for col in ['ä¸€çº§åŒºåŸŸ', 'äºŒçº§åŒºåŸŸ', 'å°åŒºåç§°']:
            if col not in df.columns:
                df[col] = ''
        priority_cols = ['ä¸€çº§åŒºåŸŸ', 'äºŒçº§åŒºåŸŸ', 'å°åŒºåç§°', 'ä»·æ ¼(å…ƒ)', 'é¢ç§¯(ã¡)', 'æˆ·å‹', 'æ ‡é¢˜']
        remaining_cols = [col for col in df.columns if col not in priority_cols]
        df = df[priority_cols + remaining_cols]

        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)
            worksheet = writer.sheets['Sheet1']
            for idx, col in enumerate(df.columns):
                max_len = max(df[col].astype(str).map(len).max(), len(col)) + 2
                worksheet.column_dimensions[chr(65 + idx)].width = min(max_len, 50)
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    except Exception as e:
        print(f"ä¿å­˜Excelå¤±è´¥: {str(e)}")
        csv_file = filename.replace('.xlsx', '.csv')
        df.to_csv(csv_file, index=False, encoding='utf_8_sig')
        print(f"å·²æ”¹ä¸ºä¿å­˜åˆ°CSVæ–‡ä»¶: {csv_file}")


# ============================================

def crawl_with_selenium():
    urls, max_pages, base_delay = load_config()
    driver = init_driver()
    if not driver:
        return

    all_data = []

    try:
        for base_url in urls:
            print(f"\nğŸš€ å¼€å§‹çˆ¬å–åŒºåŸŸ: {base_url}")
            page = 1

            for page in range(1, max_pages + 1):
                url = f"{base_url}pg{page}/"
                print(f"  â¤ è®¿é—®ç¬¬ {page} é¡µ: {url}")

                driver.get(url)
                time.sleep(2)

                # æ£€æŸ¥æ˜¯å¦è·³è½¬åˆ°éªŒè¯ç /æ‹¦æˆªé¡µ
                current_url = driver.current_url
                if "captcha" in current_url or "verify" in current_url or "unauthorized" in current_url:
                    print("âš ï¸ æ£€æµ‹åˆ°äººæœºéªŒè¯æˆ–æ‹¦æˆªé¡µé¢ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯...")
                    input("ğŸ‘‰ éªŒè¯å®Œæˆåï¼Œè¯·ç¡®ä¿å·²å›åˆ°æˆ¿æºåˆ—è¡¨é¡µï¼Œç„¶åæŒ‰å›è½¦ç»§ç»­...")

                # è§£æé¡µé¢
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                houses = soup.find_all('div', class_='content__list--item')

                if not houses:
                    print("  ğŸ“­ æœ¬é¡µæ— æˆ¿æºï¼Œæå‰ç»ˆæ­¢")
                    break

                print(f"  ğŸ“¥ è§£æåˆ° {len(houses)} æ¡æˆ¿æº")

                # æå–æ•°æ®
                for house in houses:
                    house_data = parse_house(house)
                    if house_data.get('æ ‡é¢˜'):
                        all_data.append(house_data)

                # âœ… æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæœ¬é¡µ < 30 æ¡ï¼Œè¯´æ˜æ˜¯æœ€åä¸€é¡µï¼Œåœæ­¢ç¿»é¡µ
                if len(houses) < 30:
                    print("  ğŸ›‘ æœ¬é¡µæˆ¿æºå°‘äº30æ¡ï¼Œåˆ¤å®šä¸ºæœ€åä¸€é¡µï¼Œåœæ­¢ç¿»é¡µ")
                    break

                # å»¶è¿Ÿ
                delay = base_delay + random.uniform(0.5, 1.5)
                print(f"  â³ ç­‰å¾… {delay:.1f} ç§’ååŠ è½½ä¸‹ä¸€é¡µ...")
                time.sleep(delay)

    finally:
        driver.quit()

    # ä¿å­˜ç»“æœ
    if all_data:
        df = pd.DataFrame(all_data)
        save_to_excel(df, OUTPUT_FILE)
        with open('last_file.txt', 'w', encoding='utf-8') as f:
            f.write(OUTPUT_FILE)
        print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼å…±çˆ¬å– {len(all_data)} æ¡æ•°æ®")
    else:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")


if __name__ == "__main__":
    crawl_with_selenium()